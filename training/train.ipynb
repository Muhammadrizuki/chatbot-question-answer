{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce2ba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers datasets evaluate torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fc1011",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, Trainer, TrainingArguments, TrainerCallback\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import f1_score\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset SQuAD v2\n",
    "dataset = load_dataset(\"rajpurkar/squad_v2\")\n",
    "\n",
    "# Limit the dataset to the first 100 rows for both training and validation\n",
    "# train_dataset = dataset[\"train\"].select(range(100))  # First 100 rows of the training set\n",
    "# valid_dataset = dataset[\"validation\"].select(range(100))  # First 100 rows of the validation set\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Preprocessing data\n",
    "def preprocess_function(examples):\n",
    "    questions = examples[\"question\"]\n",
    "    contexts = examples[\"context\"]\n",
    "    answers = examples[\"answers\"]\n",
    "\n",
    "    inputs = tokenizer(questions, contexts, max_length=256, truncation=True, \n",
    "                       adding=\"max_length\", return_offsets_mapping=True)\n",
    "\n",
    "    start_positions, end_positions = [], []\n",
    "    for i, offsets in enumerate(inputs[\"offset_mapping\"]):\n",
    "        if len(answers[i][\"text\"]) == 0:  # Handle no answer case in SQuAD v2\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            answer = answers[i][\"text\"][0]\n",
    "            start_char = answers[i][\"answer_start\"][0]\n",
    "            end_char = start_char + len(answer)\n",
    "\n",
    "            start_token, end_token = 0, 0\n",
    "            for j, offset in enumerate(offsets):\n",
    "                if offset[0] <= start_char and offset[1] > start_char:\n",
    "                    start_token = j\n",
    "                if offset[0] < end_char and offset[1] >= end_char:\n",
    "                    end_token = j\n",
    "\n",
    "            start_positions.append(start_token)\n",
    "            end_positions.append(end_token)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    inputs.pop(\"offset_mapping\")  # Not needed for training\n",
    "    return inputs\n",
    "\n",
    "# Tokenizing dataset\n",
    "# tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "# tokenized_valid = valid_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "# Load evaluation metrics using the evaluate library\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "exact_match_metric = evaluate.load(\"exact_match\")\n",
    "\n",
    "# Function to compute evaluation metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    start_logits, end_logits = logits  # Split the logits into start and end logits\n",
    "    start_positions, end_positions = labels  # Labels should be a tuple of (start, end) positions\n",
    "\n",
    "    # Get the predicted start and end positions\n",
    "    start_predictions = start_logits.argmax(-1)  # Start position prediction\n",
    "    end_predictions = end_logits.argmax(-1)  # End position prediction\n",
    "\n",
    "    # Compute F1 for start and end predictions\n",
    "    f1_score_start = f1_score(start_positions, start_predictions, average='macro')  # Change to 'macro' or 'micro'\n",
    "    f1_score_end = f1_score(end_positions, end_predictions, average='macro')  # Change to 'macro' or 'micro'\n",
    "\n",
    "    # Calculate Exact Match for start and end predictions\n",
    "    exact_match_start = (start_predictions == start_positions).sum() / len(start_predictions)\n",
    "    exact_match_end = (end_predictions == end_positions).sum() / len(end_predictions)\n",
    "\n",
    "    # Calculate overall F1 score and Exact Match score by averaging the start and end results\n",
    "    f1_total = (f1_score_start + f1_score_end) / 2\n",
    "    exact_match_total = (exact_match_start + exact_match_end) / 2\n",
    "\n",
    "    # Calculate overall accuracy by checking exact matches in both start and end predictions\n",
    "    accuracy = ((start_predictions == start_positions) & (end_predictions == end_positions)).sum() / len(start_predictions)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1\": f1_total,\n",
    "        \"exact_match\": exact_match_total,\n",
    "    }\n",
    "\n",
    "\n",
    "# Custom callback to print metrics after each epoch\n",
    "class PrintMetricsCallback(TrainerCallback):\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        if trainer.state.log_history:\n",
    "            last_log = trainer.state.log_history[-1]\n",
    "\n",
    "            epoch = last_log.get(\"epoch\", \"N/A\")\n",
    "            train_loss = last_log.get(\"loss\", \"N/A\")\n",
    "            val_loss = last_log.get(\"eval_loss\", \"N/A\")\n",
    "            accuracy = last_log.get(\"eval_accuracy\", \"N/A\")\n",
    "            f1_score = last_log.get(\"eval_f1\", \"N/A\")\n",
    "            exact_match = last_log.get(\"eval_exact_match\", \"N/A\")\n",
    "            learning_rate = trainer.optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "            # Print table\n",
    "            table = pd.DataFrame(\n",
    "                [[epoch, train_loss, val_loss, accuracy, f1_score, exact_match, learning_rate]],\n",
    "                columns=[\"Epoch\", \"Train Loss\", \"Val Loss\", \"Accuracy\", \"F1 Score\", \"Exact Match\", \"Learning Rate\"]\n",
    "            )\n",
    "            print(\"\\n\" + table.to_string(index=False) + \"\\n\")\n",
    "\n",
    "            # Save log history to CSV\n",
    "            df = pd.DataFrame(trainer.state.log_history)\n",
    "            df.to_csv(\"training_results.csv\", index=False)\n",
    "\n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        # Ensure the last epoch is saved to CSV\n",
    "        df = pd.DataFrame(trainer.state.log_history)\n",
    "        df.to_csv(\"training_results_end.csv\", index=False)\n",
    "\n",
    "# Training configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./distilbert-squad\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    save_steps=500,\n",
    "    warmup_steps=500,\n",
    "    fp16=True,  # Mixed precision training (if GPU supports it)\n",
    ")\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.add_callback(PrintMetricsCallback())\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save trained model\n",
    "trainer.save_model(\"./bert-squad\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
